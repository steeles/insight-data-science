{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy the powerpoetry script\n"
     ]
    },
    {
     "ename": "GetoptError",
     "evalue": "option -f not recognized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGetoptError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ac6691ff591a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Deploy the powerpoetry script\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     opts, _ = getopt.getopt(\n\u001b[0;32m--> 580\u001b[0;31m         sys.argv[1:], 'p:i:c:d', ['percentiles=', 'inquirer=', 'coca=', 'data='])\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0minputpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/my_projects_env/lib/python2.7/getopt.pyc\u001b[0m in \u001b[0;36mgetopt\u001b[0;34m(args, shortopts, longopts)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_longs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_shorts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/my_projects_env/lib/python2.7/getopt.pyc\u001b[0m in \u001b[0;36mdo_shorts\u001b[0;34m(opts, optstring, shortopts, args)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0moptstring\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mshort_has_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptstring\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/my_projects_env/lib/python2.7/getopt.pyc\u001b[0m in \u001b[0;36mshort_has_arg\u001b[0;34m(opt, shortopts)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mGetoptError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'option -%s not recognized'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGetoptError\u001b[0m: option -f not recognized"
     ]
    }
   ],
   "source": [
    "\"\"\" Creates featurespace for given iterable and ranks each feature vs the corpus characteristics.\n",
    "\n",
    "Author: Deniz Zorlu <dzorlu@sumall.org>\n",
    "\n",
    "Usage:\n",
    "    $ python pptwitter/poetry_percentile_rank.py \\\n",
    "        --data data/data.json \\\n",
    "        --coca data/coca.json \\\n",
    "        --inquirer data/inquirer.json \\\n",
    "        --percentiles data/percentiles.csv\n",
    "\n",
    "Options:\n",
    "    data: Dataset to rank.\n",
    "    percentiles: Dataset the user input will be compared to.\n",
    "    inquirer: Inquirer (Inquirer bag of words).\n",
    "    coca: COCA dictionaries.\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import getopt\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import names, wordnet\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "# Extract Features\n",
    "def ngram(poems, coca):\n",
    "    # N-gram Function\n",
    "    # Import\n",
    "    wnl = WordNetLemmatizer()\n",
    "    # Setting up the params\n",
    "    # Sentence List for Each Poem\n",
    "    sentTokenizedPoems = []\n",
    "    for i, poem in enumerate(poems):\n",
    "        # Take out the new line and replace with a space.\n",
    "        if isinstance(poem, float):\n",
    "            sentTokenizedPoems.append('')\n",
    "        else:\n",
    "            poem = re.sub('\\n', ' ', poem)\n",
    "            sentTokenizedPoems.append(sent_tokenize(poem))\n",
    "    #sentTokenizedPoems = [nltk.sent_tokenize(poem) for poem in poems]\n",
    "    # Infrequent Words - not in COCA top 20,000\n",
    "    infrequentUni = np.zeros(len(poems))\n",
    "    infrequentBi = np.zeros(len(poems))\n",
    "    infrequentTri = np.zeros(len(poems))\n",
    "    # Misspelt Word Count - not in WordNet or COCA\n",
    "    misspeltWord = np.zeros(len(poems))\n",
    "    # Unigram Frequency Count\n",
    "    unigramFreq = np.zeros(len(poems))\n",
    "    # Bigram\n",
    "    bigramFreq = np.zeros(len(poems))\n",
    "    # Trigram\n",
    "    trigramFreq = np.zeros(len(poems))\n",
    "    # Sentence Count\n",
    "    sentence_count = np.zeros(len(poems))\n",
    "    # Word Count\n",
    "    wordCount = np.zeros(len(poems))\n",
    "    logWordCount = np.zeros(len(poems))\n",
    "    # Punct Count\n",
    "    punctFreq = np.zeros(len(poems))\n",
    "\n",
    "    #COCA Dictionary\n",
    "    w1 = {tuple(re.findall('[a-zA-Z]+', item[0])): int(item[1]) for item in coca['w1']}\n",
    "    w2 = {tuple(re.findall('[a-zA-Z]+', item[0])): int(item[1]) for item in coca['w2']}\n",
    "    w3 = {tuple(re.findall('[a-zA-Z]+', item[0])): int(item[1]) for item in coca['w3']}\n",
    "\n",
    "    # calculate the frequency distribution for n-grams.\n",
    "    for i, sentTokenizedPoem in enumerate(sentTokenizedPoems):\n",
    "        # print 'Unigram: Poem ' + str(i) + ': '\n",
    "        # Tokenize each sentence in the poem\n",
    "        # Tokenized Text with PoS Tag - Tuple\n",
    "        tokenized = [nltk.pos_tag(word_tokenize(sent))\n",
    "                     for sent in sentTokenizedPoem]\n",
    "        # Sentence Count\n",
    "        sentence_count[i] = len(tokenized)\n",
    "        # Flatten the List of Tuple List\n",
    "        tokenized = list(itertools.chain(*tokenized))\n",
    "        # Remove Punctuation. Remove Numbers. Lower.\n",
    "        nonPunct = re.compile('.*[A-Za-z].*')\n",
    "        # Number of Punctuations\n",
    "        filtered = [(w[0].lower(), w[1])\n",
    "                    for w in tokenized if nonPunct.match(w[0])]\n",
    "        if len(tokenized) == 0:\n",
    "            punctFreq[i] = 0\n",
    "        else:\n",
    "            punctFreq[i] = 1 - (len(filtered) / len(tokenized))\n",
    "        # Word Count\n",
    "        wordCount[i] = len(filtered)\n",
    "        logWordCount[i] = np.log(wordCount[i])\n",
    "        # Simplify PoS in order to be able to lemmatize.\n",
    "        # Wordnet Lemmatizer knows  Adj(a), Adverb(v), Noun(n),Verb(v)\n",
    "        # Map Penn tree to wordnet for lemmatizing.\n",
    "        # Map Penn tree to COCA for frequency analysis.\n",
    "        # Lemmatized Word + COCA PoS\n",
    "        lemmatized = [(wnl.lemmatize(t[0], penn_to_wordnet(t[1])), penn_to_coca([1]))\n",
    "                      for t in filtered]\n",
    "\n",
    "        # UNIGRAM MEASUREMENT\n",
    "        nw = 0\n",
    "        for w, wordTuple in enumerate(lemmatized):\n",
    "            # Reset count\n",
    "            count = 0\n",
    "            try:\n",
    "                count = w1[wordTuple]\n",
    "                nw += 1\n",
    "                #print (wordTuple, ' found: ' + str(count))\n",
    "            except:\n",
    "            # Either the PoS convertsion from Penn is wrong.\n",
    "            # Find the word and corresponding PoS in Dict.\n",
    "                tup = []\n",
    "                if [tup for tup in w1.keys() if tup[0] == wordTuple[0]]:\n",
    "                    count = (w1[tup])\n",
    "                    nw += 1\n",
    "                    #print (wordTuple, ' appended : '+ str(count) )\n",
    "                else:\n",
    "                    # No entries in the COCA dictionary. Check if it is misspelt.\n",
    "                    # Use WordNet\n",
    "                    if wordnet.synsets(wordTuple[0]):\n",
    "                        count = 100\n",
    "                        nw += 1\n",
    "                        infrequentUni[i] += 1 / wordCount[i]\n",
    "                        #print (wordTuple, ' infrequent')\n",
    "                    # See if the word is a name. Capitalize the first Letter!\n",
    "                    # If the letter is longer than three letters.\n",
    "                    elif not (wordTuple[0].capitalize() in names.words() or len(wordTuple[0]) <= 3):\n",
    "                    # Remove from body. Add to Misspelt.\n",
    "                        misspeltWord[i] += 1 / wordCount[i]\n",
    "                        #print (wordTuple, ' misspelt?')\n",
    "                    # else:\n",
    "                        #print (wordTuple, ' not recognized.')\n",
    "            # Calculate Frequency if count different than zero.\n",
    "            if count > 0:\n",
    "                unigramFreq[i] = (1. / nw) * count + ((nw - 1.) / nw) * unigramFreq[i]\n",
    "\n",
    "        # BIGRAM FUNCTION:\n",
    "        # print 'Bigram: Poem ' + str(i) + ': '\n",
    "        # Tokenize\n",
    "        tokenized = [nltk.bigrams(word_tokenize(sentence))\n",
    "                     for sentence in sentTokenizedPoem]\n",
    "        # Flatten the List of Tuple List\n",
    "        tokenized = list(itertools.chain(*tokenized))\n",
    "        # Remove any tuple that has punctuation or number in it. Lower letter.\n",
    "        nonPunct = re.compile('.*[A-Za-z].*')\n",
    "        filtered = [(w[0].lower(), w[1].lower())\n",
    "                    for w in tokenized if nonPunct.match(w[0]) and nonPunct.match(w[1])]\n",
    "\n",
    "        # Number of words for the loop\n",
    "        nw = 0\n",
    "        for w, wordTuple in enumerate(filtered):\n",
    "            # Reset count\n",
    "            count = 0\n",
    "            try:\n",
    "                count = w2[wordTuple]\n",
    "                nw += 1\n",
    "                #print (wordTuple, ' found: ' + str(count))\n",
    "            except:\n",
    "                # Check if words exists in WordNet\n",
    "                if (wordnet.synsets(wordTuple[0])) and (wordnet.synsets(wordTuple[1])):\n",
    "                    infrequentBi[i] += 1 / wordCount[i]\n",
    "                    nw += 1\n",
    "                    count = 10\n",
    "                    #print (wordTuple, ' is infrequent')\n",
    "            # Append Frequency\n",
    "            if count > 0:\n",
    "                bigramFreq[i] = (1. / nw) * count + ((nw - 1.) / nw) * bigramFreq[i]\n",
    "\n",
    "        # TRIGRAMS\n",
    "        # print 'Trigram: Poem ' + str(i) + ': '\n",
    "        # Tokenize\n",
    "        tokenized = [nltk.trigrams(word_tokenize(sentence))\n",
    "                     for sentence in sentTokenizedPoem]\n",
    "        # Flatten the List of Tuple List\n",
    "        tokenized = list(itertools.chain(*tokenized))\n",
    "        # Remove any tuple that has punctuation or number in it. Lower letter.\n",
    "        nonPunct = re.compile('.*[A-Za-z].*')\n",
    "        filtered = [\n",
    "            (w[0].lower(), w[1].lower(), w[2].lower()) for w in tokenized\n",
    "            if nonPunct.match(w[0]) and nonPunct.match(w[1]) and nonPunct.match(w[2])]\n",
    "\n",
    "        # Number of words for the loop\n",
    "        nw = 0\n",
    "        for w, wordTuple in enumerate(filtered):\n",
    "            # Reset count\n",
    "            count = 0\n",
    "            try:\n",
    "                # print wordTuple\n",
    "                count = w3[wordTuple]\n",
    "                nw += 1\n",
    "                #print (wordTuple, ' found: ' + str(count))\n",
    "            except:\n",
    "                # Check if words exists in WordNet\n",
    "                word_exists = lambda idx: wordnet.synsets(wordTuple[idx])\n",
    "                if word_exists(0) and word_exists(1) and word_exists(2):\n",
    "                    infrequentTri[i] += 1 / wordCount[i]\n",
    "                    nw += 1\n",
    "                    count = 10\n",
    "                    #print (wordTuple, ' is infrequent')\n",
    "            # Append Frequency\n",
    "            if count > 0:\n",
    "                trigramFreq[i] = (1. / nw) * count + ((nw - 1.) / nw) * trigramFreq[i]\n",
    "\n",
    "        # Log Frequnecy\n",
    "        bigramFreq[i] = np.log(bigramFreq[i])\n",
    "        trigramFreq[i] = np.log(trigramFreq[i])\n",
    "        unigramFreq[i] = np.log(unigramFreq[i])\n",
    "\n",
    "    return (\n",
    "        unigramFreq, bigramFreq, trigramFreq, misspeltWord,\n",
    "        sentence_count, logWordCount, punctFreq\n",
    "    )\n",
    "\n",
    "\n",
    "# Utilities\n",
    "# Treebank PoS to WordNet in order to lemmatize\n",
    "# Assign Noun if none of the conditions are satisfied.\n",
    "def penn_to_wordnet(treebank_tag):\n",
    "    from nltk.corpus import wordnet\n",
    "    #\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def penn_to_coca(treebank_tag):\n",
    "    # Mapping from Penn Tree PoS to COCA PoS\n",
    "    mapCOCA = {'-NONE-': 'n',\n",
    "               '': 'n',\n",
    "               'CC': 'c',\n",
    "               'CD': 'm',\n",
    "               'DT': 'a',\n",
    "               'EX': 'e',\n",
    "               'FW': 'f',\n",
    "               'IN': 'i',\n",
    "               'JJ': 'j',\n",
    "               'JJR': 'j',\n",
    "               'JJS': 'j',\n",
    "               'LS': 'g',\n",
    "               'MD': 'v',\n",
    "               'NN': 'n',\n",
    "               'NNP': 'n',\n",
    "               'NNPS': 'n',\n",
    "               'NNS': 'n',\n",
    "               'PDT': 'd',\n",
    "               'POS': 'd',\n",
    "               'PRP': 'p',\n",
    "               'PRP$': 'p',\n",
    "               'RB': 'r',\n",
    "               'RBR': 'r',\n",
    "               'RBS': 'r',\n",
    "               'RP': 'r',\n",
    "               'SYM': 'v',\n",
    "               'TO': 't',\n",
    "               'UH': 'u',\n",
    "               'VB': 'v',\n",
    "               'VBD': 'v',\n",
    "               'VBG': 'v',\n",
    "               'VBN': 'v',\n",
    "               'VBP': 'v',\n",
    "               'VBZ': 'v',\n",
    "               'WDT': 'd',\n",
    "               'WP': 'p',\n",
    "               'WP$': 'p',\n",
    "               'WRB': 'r'}\n",
    "    try:\n",
    "        mapped = mapCOCA[treebank_tag]\n",
    "    except:\n",
    "        # If Penn Tree Mapping not mapped explictly, return 'noun'\n",
    "        mapped = 'n'\n",
    "    return(mapped)\n",
    "\n",
    "\n",
    "def sound(poems):\n",
    "    prondict = nltk.corpus.cmudict.dict()\n",
    "    # Records instances of sound devices.\n",
    "    nonPunct = re.compile('.*[A-Za-z].*')\n",
    "    # Split by new line\n",
    "    # Convert Float\n",
    "    sentTokenizedPoems = []\n",
    "    for poem in poems:\n",
    "        if isinstance(poem, float):\n",
    "            sentTokenizedPoems.append('')\n",
    "        else:\n",
    "            sentTokenizedPoems.append(poem.split('\\n'))\n",
    "    #sentTokenizedPoems = [poem.split('\\r\\n') for poem in poems]\n",
    "    # Perfect Rhyme,Slant Rhyme, Alliteration, Consonance, Assonance\n",
    "    perfectRhymeFreq = np.zeros(len(poems))\n",
    "    slantRhymeFreq = np.zeros(len(poems))\n",
    "    alliterFreq = np.zeros(len(poems))\n",
    "    # consonFreq=np.zeros(len(poems))\n",
    "    # assonFreq=np.zeros(len(poems))\n",
    "    # Take the words at the end of the line, ignoring punctuations\n",
    "    for i, sentTokenizedPoem in enumerate(sentTokenizedPoems):\n",
    "        # Create the lists for Alliteration and Rhymes.\n",
    "        lastWords = []\n",
    "        # tokenize  the word list in each sentence.\n",
    "        tokenized = [word_tokenize(sent) for sent in sentTokenizedPoem]\n",
    "        #total_words = len(list(itertools.chain(*tokenized)))\n",
    "        # Excluding punctuations\n",
    "        total_words = len(\n",
    "            [w for w in list(itertools.chain(*tokenized)) if nonPunct.match(w)])\n",
    "        ######################################################\n",
    "        ####Measure the Alliteration, Consonance, Assonance###\n",
    "        ######################################################\n",
    "        # Alliteration:Find the first phoneme of first pronounciation of each word.\n",
    "        # Consonance: Find the matching consonant phenomes.\n",
    "        # Assonance: Find the matching vowel phenomes.\n",
    "        for sent in tokenized:\n",
    "        # Sentence by Sentence\n",
    "            phenomes = []\n",
    "            for word in sent:\n",
    "                # Retrieve the Phenomes\n",
    "                try:\n",
    "                    phenomes.append(prondict[word.lower()][0])\n",
    "                except KeyError:\n",
    "                    logger.debug('item: %s not found in CMU Dictionary', word)\n",
    "                    phenomes.append('')\n",
    "            # Alliteration: Find the first phenome.\n",
    "            # Start from the second unit to match the consecutive ones.\n",
    "            for n, phon in enumerate(phenomes[1:]):\n",
    "                # Making sure (i) cons first ph match (ii) cons (iii) not NaN.\n",
    "                nextfp = phenomes[n]\n",
    "                lastfp = phenomes[n - 1]\n",
    "                # print nextfp, lastfp\n",
    "                if nextfp and lastfp:\n",
    "                    # First Phenome for consecutive words. Hence [0]\n",
    "                    if nextfp[0] in lastfp[0] and not re.search(r'\\d+', nextfp[0]):\n",
    "                        logger.debug('Alliteration match: %s %s', nextfp, lastfp)\n",
    "                        alliterFreq[i] += (1. / total_words)\n",
    "\n",
    "        ###################################################\n",
    "        ###########Perfect and Slant Rhymes################\n",
    "        ##################################################\n",
    "        # Find the last word in each line\n",
    "        for line in tokenized:\n",
    "            for z, word in enumerate(line):\n",
    "                if nonPunct.match(line[-(1 + z)]):\n",
    "                    # Lower\n",
    "                    lastWords.append(line[-(1 + z)].lower())\n",
    "                    break\n",
    "        # Retrieve the pronounciation from CMU Dictionary\n",
    "        #[a for a in pdict if a[0] in 'world']\n",
    "        plist = []\n",
    "        for word in lastWords:\n",
    "            try:\n",
    "                # The fist pronounciation.\n",
    "                plist.append(prondict[word][0])\n",
    "            except KeyError:\n",
    "                logger.debug('item: %s not found in CMU Dictionary.', word)\n",
    "                plist.append([])\n",
    "        # Divide the pronounciation of last words into rolling windows of 4 .\n",
    "        window_length = 4\n",
    "        windows = [plist[n:n + window_length]\n",
    "                   for n, p in enumerate(plist) if n + window_length <= len(plist)]\n",
    "        # Perfect and Slant Ryhmes within rolling windows\n",
    "        for window in windows:\n",
    "            # For each window go through the lines and match other lines.\n",
    "            # End the loop at window[:-1] to avoid duplicates.\n",
    "            # eg. 1 vs 2,3,4; 2 vs 3,4; 3 vs 4.\n",
    "            for x, line in enumerate(window[:-1]):\n",
    "                # Stressed Vowel Phoneme: Ends with 1. Tuple. Last one.\n",
    "                # If there is no stressed vowel e.g. and. skip the line.\n",
    "                # because rhymes depend on the existince of stressed vowel.\n",
    "                vpprim = [(n, p)\n",
    "                          for n, p in enumerate(line) if re.search('1', p)]\n",
    "                if vpprim:\n",
    "                    # Primary Initial Phoneme\n",
    "                    ipprim = line[0]\n",
    "                    # phoneme sequences from the stressed vowel phoneme onward.\n",
    "                    # else stressed vowel phoneme is the last phoneme\n",
    "                    # vpprim[-1][1] is to access the last primary vowel.\n",
    "                    if vpprim[-1][1] not in line[-1]:\n",
    "                        spprim = line[1 + vpprim[-1][0]:]\n",
    "                    else:\n",
    "                        spprim = [vpprim[-1][1]]\n",
    "                    # Loop Remaning Lines to match the conditions.\n",
    "                    # Secondary. Match starts from x+1 on to avoid duplicates.\n",
    "                    for y, match in enumerate(window[x + 1:]):\n",
    "                        vpsec = [(n, p)\n",
    "                                 for n, p in enumerate(match) if re.search('1', p)]\n",
    "                        if vpsec:\n",
    "                            # Secondary Initial Phoneme.\n",
    "                            ipsec = match[0]\n",
    "                            # Phoneme seq from the stressed vowel on\n",
    "                            # else stressed vowel phoneme is the last phoneme\n",
    "                            if vpsec[-1][1] not in match[-1]:\n",
    "                                spsec = match[1 + vpsec[-1][0]:]\n",
    "                            else:\n",
    "                                spsec = [vpsec[-1][1]]\n",
    "                            # Perfect Rhyme and Slant Rhyme\n",
    "                            # Different initial consonants.\n",
    "                            # Matching stressed vowel phoneme:\n",
    "                            # Matching phoneme sequences after vowel.\n",
    "                            # Matching the last phoneme\n",
    "                            cona = (ipsec not in ipprim)\n",
    "                            conb = (vpsec[-1][1] in vpprim[-1][1])\n",
    "                            conc = (spprim == spsec)\n",
    "                            cond = (spprim[-1] == spsec[-1])\n",
    "                            # Perfect Rhyme\n",
    "                            if cona and conb and conc:\n",
    "                                perfectRhymeFreq[i] += (1. / len(windows[0]))\n",
    "                                # print 'Perfect:  ',vpprim, vpsec, spprim, spsec\n",
    "                            # Slant Rhyme\n",
    "                            if conb ^ cond:\n",
    "                                slantRhymeFreq[i] += (1. / len(windows[0]))\n",
    "                                # print 'Slant: ',vpprim, vpsec, spprim, spsec\n",
    "\n",
    "    return(perfectRhymeFreq, slantRhymeFreq, alliterFreq)\n",
    "\n",
    "\n",
    "def sentiment(poems, inquirer):\n",
    "    # Sentiment Analysis based on bag of words in Harvard Inquirer dictionary.\n",
    "    # Poems are iterable object, inquirer is the dictionary.\n",
    "\n",
    "    wnl = WordNetLemmatizer()\n",
    "    # Records instances of sound devices.\n",
    "    nonPunct = re.compile('.*[A-Za-z].*')\n",
    "    # Sentence List for Each Poem\n",
    "    sentPoems = []\n",
    "    for poem in poems:\n",
    "        # Take out the new line and replace with a space.\n",
    "        if isinstance(poem, float):\n",
    "            sentPoems.append('')\n",
    "        else:\n",
    "            poem = re.sub('\\n', ' ', poem)\n",
    "            sentPoems.append(nltk.sent_tokenize(poem))\n",
    "    #sentPoems = [nltk.sent_tokenize(poem) for poem in poems]\n",
    "    # Perfect Rhyme,Slant Rhyme, Alliteration, Consonance, Assonance\n",
    "    PosNeg = np.zeros(len(poems))\n",
    "    ABS = np.zeros(len(poems))\n",
    "    EnlTot = np.zeros(len(poems))\n",
    "    Female = np.zeros(len(poems))\n",
    "    Male = np.zeros(len(poems))\n",
    "    Female = np.zeros(len(poems))\n",
    "    Object = np.zeros(len(poems))\n",
    "    Polit = np.zeros(len(poems))\n",
    "    Race = np.zeros(len(poems))\n",
    "    Relig = np.zeros(len(poems))\n",
    "    St = np.zeros(len(poems))\n",
    "    WlbPhycs = np.zeros(len(poems))\n",
    "    WlbPsyc = np.zeros(len(poems))\n",
    "\n",
    "    # Take the words at the end of the line, ignoring punctuations\n",
    "    for i, sentPoem in enumerate(sentPoems):\n",
    "        logger.debug('sentence: %s', sentPoem)\n",
    "        # Create the lists for Alliteration and Rhymes.\n",
    "        # tokenize  the word list in each sentence.\n",
    "        tokenized = [nltk.pos_tag(word_tokenize(sent)) for sent in sentPoem]\n",
    "        #total_words = len(list(itertools.chain(*tokenized)))\n",
    "        total_words = len([(w[0].lower(), w[1])\n",
    "                          for w in list(itertools.chain(*tokenized)) if nonPunct.match(w[0])])\n",
    "        ##########################\n",
    "        ####Sentiment Analysis###\n",
    "        ##########################\n",
    "        #Negation String####\n",
    "        negation = re.compile(\n",
    "            'n\\'t|never|no|nothing|signnowhere|noone|none|not|havent|'\n",
    "            'hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|'\n",
    "            'didnt|isnt|arent|aint')\n",
    "        punct = re.compile('^[,.:;!?()]$|that')\n",
    "        for sent in tokenized:\n",
    "        # Sentence by Sentence\n",
    "        # Create an array to measure the negation in a given sentence.\n",
    "            # negationArray=np.zeros(len(sent))\n",
    "            negate = np.ones(1)\n",
    "            for wordTuple in sent:\n",
    "                # Lemmatize to do the sentiment count.\n",
    "                word = wnl.lemmatize(\n",
    "                    wordTuple[0].lower(),\n",
    "                    penn_to_wordnet(wordTuple[1]))\n",
    "                # print word\n",
    "                # Flip the negation sign.\n",
    "                if negation.match(word):\n",
    "                    negate = (-1) * negate\n",
    "                # Reset negation. If punctuation, do not match the word below.\n",
    "                if punct.match(word):\n",
    "                    negate = np.ones(1)\n",
    "                elif word in inquirer.keys():\n",
    "                    # Word Match. Except Negative + Positive\n",
    "                    wm = inquirer[word]\n",
    "                    ABS[i] += int(wm['ABS']) / total_words\n",
    "                    EnlTot[i] += int(wm['EnlTot']) / total_words\n",
    "                    Female[i] += int(wm['Female']) / total_words\n",
    "                    Male[i] += int(wm['MALE']) / total_words\n",
    "                    Object[i] += int(wm['Object']) / total_words\n",
    "                    Polit[i] += int(wm['POLIT']) / total_words\n",
    "                    Race[i] += int(wm['Race']) / total_words\n",
    "                    Relig[i] += int(wm['Relig']) / total_words\n",
    "                    St[i] += int(wm['St']) / total_words\n",
    "                    WlbPhycs[i] += int(wm['WlbPhys']) / total_words\n",
    "                    WlbPsyc[i] += int(wm['WlbPsyc']) / total_words\n",
    "                    # Positive\n",
    "                    PosNeg[i] += negate * int(wm['PosNeg']) / total_words\n",
    "                    # print negate, int(wm['PosNeg']), PosNeg[i]\n",
    "                    #PosNeg[i] =  PosNeg[i]/total_words\n",
    "        logger.debug('Sentiment Score is %s', PosNeg[i])\n",
    "    return (ABS, EnlTot, Female, Male, Object, Polit, Race, Relig, St, WlbPhycs, WlbPsyc, PosNeg)\n",
    "\n",
    "\n",
    "class PercentilePoetryRanker(object):\n",
    "\n",
    "    columns = [\n",
    "        'perfectRhymeFreq', 'slantRhymeFreq', 'alliterFreq', 'Polit', 'Race', 'Relig',\n",
    "        'PosNeg', 'unigramFreq', 'bigramFreq', 'trigramFreq', 'misspeltWord', 'sentence_count',\n",
    "        'wordCount', 'punctFreq'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, inquirer_filename, coca_filename, percentiles_filename):\n",
    "        self.percentiles = pd.read_csv(percentiles_filename)\n",
    "        self.benchmarks = self.percentiles\n",
    "\n",
    "        with open(inquirer_filename) as f:\n",
    "            self.inquirer = json.loads(f.read())\n",
    "\n",
    "        with open(coca_filename) as f:\n",
    "            self.coca = json.loads(f.read())\n",
    "\n",
    "    def percentile_subset(self, data):\n",
    "        #Sentiment - Descending\n",
    "        sentiment = -data['PosNeg']\n",
    "        #Language Mastery - ngrams, misspeltword. All Ascending\n",
    "        language = data[['unigramFreq', 'bigramFreq', 'trigramFreq', 'misspeltWord']].mean(axis=1)\n",
    "        #Poetic - perfect, slant, alliteration. Descending\n",
    "        poetic = -data[['perfectRhymeFreq', 'slantRhymeFreq', 'alliterFreq']].mean(axis=1)\n",
    "        #Save to File\n",
    "        percentile = pd.DataFrame([sentiment, language, poetic]).T\n",
    "        percentile.columns = ['sentiment', 'language', 'poetic']\n",
    "        return percentile\n",
    "\n",
    "    def rank(self, data):\n",
    "        ABS, EnlTot, Female, Male, Object, Polit, Race, Relig, St, WlbPhycs, WlbPsyc, PosNeg = \\\n",
    "            sentiment(data, self.inquirer)\n",
    "\n",
    "        unigramFreq, bigramFreq, trigramFreq, misspeltWord, sentence_count, \\\n",
    "            logWordCount, punctFreq = \\\n",
    "            ngram(data, self.coca)\n",
    "\n",
    "        perfectRhymeFreq, slantRhymeFreq, alliterFreq = \\\n",
    "            sound(data)\n",
    "\n",
    "        # Output just few features\n",
    "        features = pd.DataFrame([\n",
    "            perfectRhymeFreq, slantRhymeFreq, alliterFreq, Polit, Race, Relig,\n",
    "            PosNeg, unigramFreq, bigramFreq, trigramFreq, misspeltWord, sentence_count,\n",
    "            logWordCount, punctFreq\n",
    "        ]).transpose()\n",
    "        features.columns = self.columns\n",
    "\n",
    "        return ({\n",
    "            k: self.rank_feature(k, v) for k, v in f.iteritems()\n",
    "        } for _, f in self.percentile_subset(features).iterrows())\n",
    "\n",
    "    def rank_feature(self, feature, value):\n",
    "        return int(sp.stats.percentileofscore(self.benchmarks[feature], value))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print \"Deploy the powerpoetry script\"\n",
    "    opts, _ = getopt.getopt(\n",
    "        sys.argv[1:], 'p:i:c:d', ['percentiles=', 'inquirer=', 'coca=', 'data='])\n",
    "\n",
    "    inputpath = None\n",
    "    for opt, arg in opts:\n",
    "        if opt in ('-p', '--percentiles'):\n",
    "            percentiles_filename = arg\n",
    "        elif opt in ('-i', '--inquirer'):\n",
    "            inquirer_filename = arg\n",
    "        elif opt in ('-c', '--coca'):\n",
    "            coca_filename = arg\n",
    "        elif opt in ('-d', '--data'):\n",
    "            keypath = arg\n",
    "\n",
    "    with open(inputpath) as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    ranker = PercentilePoetryRanker(\n",
    "        inquirer_filename,\n",
    "        coca_filename,\n",
    "        percentiles_filename)\n",
    "    print list(ranker.rank(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
